<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"waline","storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="乙巳年伊始，DeepSeek R1 的开源犹如一响惊雷，震动了寰宇大地，由 GPT 引领的大模型演进、自然语言处理、神经网络算法乃至深度学习、全图谱人工智能领域的热度再次被推上一个新的高度。新一年几乎掀起了全民学 AI 的热潮，似乎预示着颠覆性变革的到来。">
<meta property="og:type" content="article">
<meta property="og:title" content="大话 LLM">
<meta property="og:url" content="http://example.com/2025/02/17/llm-intro/index.html">
<meta property="og:site_name" content="Raymond&#39;s Cabin">
<meta property="og:description" content="乙巳年伊始，DeepSeek R1 的开源犹如一响惊雷，震动了寰宇大地，由 GPT 引领的大模型演进、自然语言处理、神经网络算法乃至深度学习、全图谱人工智能领域的热度再次被推上一个新的高度。新一年几乎掀起了全民学 AI 的热潮，似乎预示着颠覆性变革的到来。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/Neural-Networks-Architecture.png">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/activation-functions.png">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/loss-function.png">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/Schematic-of-backpropagation-In-the-forward-pass-of-a-simple-neural-network-an-input.ppm">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/dropout.png">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/What-is-Recurrent-Neural-Network-660.webp">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/lstm.png">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/encoder_decoder2.webp">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/attention.png">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/The-transformer-encoder-decoder-model.png">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/Attention_diagram_transformer.webp">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/padding%20mask.png">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/decoder-only.png">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/Supervised_and_unsupervised_learning.png">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/Zero-Shot-Learning-in-NLP-Modulai.webp">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/multask.png">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/scaling-law.png">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/meta-learning.jpg">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/in-context.jpg">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/prompt.png">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/reinforcement-learning.png">
<meta property="og:image" content="http://example.com/2025/02/17/llm-intro/mla.jpg">
<meta property="article:published_time" content="2025-02-17T11:39:54.000Z">
<meta property="article:modified_time" content="2025-02-20T12:57:57.890Z">
<meta property="article:author" content="敖惠竣｜Ao Huijun, Raymond">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2025/02/17/llm-intro/Neural-Networks-Architecture.png">

<link rel="canonical" href="http://example.com/2025/02/17/llm-intro/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>大话 LLM | Raymond's Cabin</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Raymond's Cabin</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">for sharing thoughts</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/daca-ao" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/17/llm-intro/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="敖惠竣｜Ao Huijun, Raymond">
      <meta itemprop="description" content="Storing blogs">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Raymond's Cabin">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          大话 LLM
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-02-17 19:39:54" itemprop="dateCreated datePublished" datetime="2025-02-17T19:39:54+08:00">2025-02-17</time>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>乙巳年伊始，DeepSeek R1 的开源犹如一响惊雷，震动了寰宇大地，由 GPT 引领的大模型演进、自然语言处理、神经网络算法乃至深度学习、全图谱人工智能领域的热度再次被推上一个新的高度。新一年几乎掀起了全民学 AI 的热潮，似乎预示着颠覆性变革的到来。</p>
<span id="more"></span>
<p>硕士生涯的时候学习过 AI 基础，加上本人的风格就是在遇到一个新风口的时候喜欢探究其原理，于是乎，想借助本文简单拟一个 DeepSeek，乃至整个大模型演进的科普文章，简单阐述原理，以便在日后的工作中能够根据基础知识举一反三、触类旁通、事半功倍。</p>
<p>我们先从 DeepSeek 的基础 —— 深度学习简单展开，然后聊一下 DeepSeek 的竞对 GPT，再通过对比引出 DeepSeek 的精妙之处。</p>
<h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><h2 id="普通的神经网络"><a href="#普通的神经网络" class="headerlink" title="普通的神经网络"></a>普通的神经网络</h2><p>一个基础的神经网络包括三层：<strong>输入层</strong>、<strong>隐藏层</strong>和<strong>输出层</strong></p>
<p><img src="/2025/02/17/llm-intro/Neural-Networks-Architecture.png" alt></p>
<ul>
<li>输入层自身就是一层对接输入数据的神经网络；</li>
<li>输出层输出整个神经网络的计算结果；</li>
<li>隐藏层涉及到功能执行的参数和计算过程，不直接暴露，即“黑盒”</li>
</ul>
<p>“黑盒”的实现有很多，最简单的是单层感知机（Single-layer Perceptron），用于二分类任务的线性分类器（binary classifier），无法处理非线性问题；其它常用的神经网络包括：</p>
<ul>
<li>CNN(Convolutional Neural Network) 卷积神经网络 </li>
<li>GNN(Graph Neural Network) 图神经网络</li>
<li>RNN(Recurrent Neural Network) 循环神经网络</li>
</ul>
<h2 id="输入预处理-pre-processing"><a href="#输入预处理-pre-processing" class="headerlink" title="输入预处理 pre-processing"></a>输入预处理 pre-processing</h2><p>并不是所有的数据，都能通过隐藏层输出符合预期的结果；隐藏层往往对输入有明显的特征要求，所以对输入数据进行预处理非常重要。</p>
<h2 id="激活函数-activation-function"><a href="#激活函数-activation-function" class="headerlink" title="激活函数 activation function"></a>激活函数 activation function</h2><p>激活函数是神经网络的重要特征，决定了网络中的<strong>一个神经元是否应该被激活</strong>：神经元接收的信息与给定的条件有关。</p>
<p>激活函数对输入信息进行<strong>非线性变换</strong>，变换后的输出会作为下一个神经元的输入 —— 这个变换非常重要，如果没有激活函数，输出永远都是基于输入的线性转换，极有可能无法收敛于我们的期望结果。</p>
<p>因此激活函数存在于<strong>输入层、隐藏层和输出层的所有神经元</strong>，简单的激活函数有：</p>
<ul>
<li><code>sigmoid</code>: 适合输出预测概率，存在梯度消失（函数图像的两端，即输入值很大或很小的情况下，导数接近于 0 —— 意味着在网络的深层，由于多次连乘的影响，梯度会变得极小，导致演进停滞不前）可能；</li>
<li><code>tanh</code>: 适合输出预测概率，存在梯度消失可能；</li>
<li><code>relu</code>: 无梯度消失，几乎可以到处使用，但表现能力较弱，一般只用于隐藏层；</li>
<li><code>softmax</code>: 目前最常用，多用于<strong>多分类</strong>问题，存在梯度消失可能</li>
</ul>
<p><img src="/2025/02/17/llm-intro/activation-functions.png" alt></p>
<h2 id="损失函数-loss-function"><a href="#损失函数-loss-function" class="headerlink" title="损失函数 loss function"></a>损失函数 loss function</h2><p>损失函数用来度量模型的预测值 <code>f(x)</code> 和真实值 <code>Y</code> 的差异程度，是一个<strong>非负实值函数</strong>：损失函数越小，模型越强壮。</p>
<p><img src="/2025/02/17/llm-intro/loss-function.png" alt></p>
<p>损失函数主要用在模型的训练阶段：</p>
<ol>
<li>每个批次的训练数据被送入模型后，通过<strong>前向传播</strong>（forward pass）输出预测值；</li>
<li>损失函数根据预测值算出和真实值的损失值，通过<strong>反向传播</strong>（backward pass）更新模型的各个参数，以图降低后续的损失值；</li>
<li>由此往复，使模型预测值向真实值方向收敛，达到学习目的</li>
</ol>
<p><img src="/2025/02/17/llm-intro/Schematic-of-backpropagation-In-the-forward-pass-of-a-simple-neural-network-an-input.ppm" alt></p>
<p>因为神经网络乃至 AI 中有两个不同的场景：<strong>训练</strong>和<strong>推理</strong></p>
<ul>
<li>推理的计算过程仅包括前向传播：因为整个神经网络模型已经准备完毕，用户输入数据后可以获得相对稳定且准确的结果；</li>
<li>训练的计算过程都包括：因为此时的神经元参数为 0 或其它参数值，用户输入数据后并不能获得准确结果，需要根据损失函数的损失值，通过监督（supervised learning）或无监督（unsupervised learning）学习等进行反向传播，<strong>更新参数值</strong>，并不停地进行迭代，使模型可用于推理</li>
</ul>
<p>因此训练的计算量更大，内存访问更多，过程更复杂。</p>
<p>参数更新的方式也很简单：<strong>求导</strong>，获得梯度，以梯度的更新向合理的参数逼近。梯度更新注意避免两种情况：</p>
<ul>
<li><strong>梯度消失</strong>：梯度为 0 或接近于 0，后果是参数无更新，或更新极小，导致训练缓慢甚至停止（所谓的“神经元无法有效传导”）；</li>
<li><strong>梯度爆炸</strong>：梯度过大，参数更新过于激烈，导致损失函数一直抖动，无法收敛</li>
</ul>
<h2 id="解决过度拟合-overfitting"><a href="#解决过度拟合-overfitting" class="headerlink" title="解决过度拟合 overfitting"></a>解决过度拟合 overfitting</h2><p>类似于数据挖掘的经典问题：避免陷入局部最优解。过度拟合体现在：模型在训练集上损失值较低，预测准确率较高；但在测试集上损失值大，预测准确率低，得到的模型基本不可用。</p>
<p>常用的有一种缓解过度拟合的方法：dropout，要求所有神经元带上一个停止工作的概率 p，在前向传播的时候，某几个神经元会有一定的概率 p 停止工作，使模型不太依赖某些局部特征，更具有泛化性。</p>
<p><img src="/2025/02/17/llm-intro/dropout.png" alt></p>
<h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><p>自然语言处理（Natural Language Processing, NLP）是人工智能领域的一个重要方向，大模型的本身也是 NLP 的一部分，在 GPT 实现多模态之前，输入也是文本语言。</p>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>循环神经网络 RNN 是最早用于 NLP 的“黑盒”构型，<strong>不仅依赖当前输入，还取决于先前输入的中间结果</strong>。</p>
<p><img src="/2025/02/17/llm-intro/What-is-Recurrent-Neural-Network-660.webp" alt></p>
<p>因此 RNN 可以通过先前输入的中间结果影响当前输入的结果，与 NLP 的基本问题（根据先前的文本推断下文）高度吻合；故同样类型的推理（训练）任务也可以用 RNN 处理。</p>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>Long Short-Term Memory，长短期记忆属于一种特殊的 RNN 网络，主要解决了长序列训练过程中的梯度消失和梯度爆炸问题，在更长序列中有更好的表现。</p>
<p>相比于 RNN 只有一个传递状态，LSTM 有两个传递状态，cell state 和 hidden state：</p>
<ul>
<li>传递的 cell state 改变得很慢，通常的输出是上一状态的 cell state 加上一些数值；</li>
<li>hidden state 在不同的节点下往往有很大区别</li>
</ul>
<p><img src="/2025/02/17/llm-intro/lstm.png" alt></p>
<p>因为 cell state 的存在，LSTM 对长期信息的记忆更久。</p>
<h2 id="Encoder-decoder"><a href="#Encoder-decoder" class="headerlink" title="Encoder-decoder"></a>Encoder-decoder</h2><p>以上情况我们均考虑输入输出序列等长的情况，然而实际生活中大量存在输入输出序列长度不等的情况，如机器翻译、语音识别、问答系统等。</p>
<p>Encoder-decoder 能够完成从一个可变长序列至另一个可变长序列的映射，属于机器模型（Machine Translation）的产物。</p>
<p>它的基本思想非常简单：<strong>使用一个 RNN 读取输入的句子，将整个句子的信息“压缩”到一个固定维度的编码中；再使用另一个 RNN 读取这个编码，将其“解压”为目标语言的一个句子</strong>（seq2seq）。由此的“压缩”和“解压”一来一回，就组成了一个编码器（Encoder）和解码器（Decoder），大大提高灵活性。</p>
<p><img src="/2025/02/17/llm-intro/encoder_decoder2.webp" alt></p>
<p>但是它的局限性也很明显：encoder 和 decoder 之间的唯一联系是一个固定的语义编码 C，即 encoder 要将整个序列的信息压缩到一个固定长度变量中。</p>
<ol>
<li>一是语义编码 C 可能无法完全表示整个序列的信息；</li>
<li>二是先输入到网络的内容携带的信息会被后输入的信息覆盖，输入序列越长，现象越严重</li>
</ol>
<p>这两个弊端会导致解码的时候，decoder 一开始就没有获得序列足够多的信息，导致解码准确率不高。</p>
<p>为了解决这两个弊端，大牛们引入了著名的注意力（attention）机制。</p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>深度学习中的注意力机制借鉴了人类的注意力思维方式。在最早提出 attention 的论文中，attention 是作为一个中间层出现的，位于 encoder 和 decoder 之间，跟原来的语义编码类似。</p>
<p>我们可以从两个方面简单理解 attention：</p>
<ol>
<li>每次处理（每次只注意）一个窗口的输入数据；</li>
<li>注重数据之间的关联性</li>
</ol>
<p>在 attention 中，语义编码不再是固定维度的编码，也不是输入序列的直接编码，而是由各个元素按照重要程度加权求和得到的值：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="14.378ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 6354.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"/></g><g data-mml-node="mi" transform="translate(748,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(1319.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="msub" transform="translate(2375.5,0)"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"/></g><g data-mml-node="mi" transform="translate(783,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(3674.7,0)"><path data-c="B7" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"/></g><g data-mml-node="mi" transform="translate(4174.9,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mo" transform="translate(4724.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(5113.9,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"/></g><g data-mml-node="mo" transform="translate(5965.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container>。</p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.437ex" height="1.977ex" role="img" focusable="false" viewbox="0 -716 1077 873.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"/></g><g data-mml-node="mi" transform="translate(783,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container> 和 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.932ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 2180 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mo" transform="translate(550,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(939,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"/></g><g data-mml-node="mo" transform="translate(1791,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container> 均为输入序列的长度，其中 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.437ex" height="1.977ex" role="img" focusable="false" viewbox="0 -716 1077 873.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"/></g><g data-mml-node="mi" transform="translate(783,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container> 的计算公式：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="31.509ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 13926.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"/></g><g data-mml-node="mi" transform="translate(783,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(1354.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(2410.5,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/></g><g data-mml-node="mi" transform="translate(2879.5,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"/></g><g data-mml-node="mi" transform="translate(3364.5,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mi" transform="translate(3914.5,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mi" transform="translate(4275.5,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(5153.5,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mi" transform="translate(5682.5,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mo" transform="translate(6254.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(6643.5,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/></g><g data-mml-node="mi" transform="translate(7112.5,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"/></g><g data-mml-node="mi" transform="translate(7545.5,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"/></g><g data-mml-node="mi" transform="translate(8030.5,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(8481.5,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"/></g><g data-mml-node="mo" transform="translate(8947.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(9336.5,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mo" transform="translate(9886.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(10275.5,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"/></g><g data-mml-node="mo" transform="translate(11127.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(11738.7,0)"><path data-c="B7" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"/></g><g data-mml-node="msub" transform="translate(12239,0)"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"/></g><g data-mml-node="mi" transform="translate(616,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(13148.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(13537.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container> ，这里 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.059ex" height="1.902ex" role="img" focusable="false" viewbox="0 -683 910 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"/></g><g data-mml-node="mi" transform="translate(616,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container> 为 decoder 的打分，基于前 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="4.677ex" height="1.692ex" role="img" focusable="false" viewbox="0 -666 2067.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(567.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1567.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g></svg></mjx-container> 次输出所得；score 函数为打分函数。 </p>
<p><img src="/2025/02/17/llm-intro/attention.png" alt></p>
<p>总体而言，attention 机制可以分为四步：</p>
<ol>
<li>点积（向量在目标向量上的投影）：值越大说明关联度越大，0 说明没有关联度</li>
<li>打分：第一步的加强，获得一个更加贴合需要的匹配分布</li>
<li>softmax；基于前两步输出做重新分布；</li>
<li>点积</li>
</ol>
<p>目前业界存在很多不同的分类方法，其中使用比较多的一个是 soft-attention，另一个是 transformer 使用的 self-attention。</p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>讲到这里终于可以展开聊聊这块大模型的基石了。由于其良好的性能，导致目前大模型的发展就是堆 transformer，算力的核心就是 transformer 的各个机制。</p>
<p>虽然 attention 机制解决了 Encoder-decoder 输入输出必须等长的问题，但本质还是在 Encoder-decoder 上雕花，执行顺序是<strong>循环顺序</strong>，也就是说，只能从左到右或者从右到左计算，这样子就存在另外的问题：</p>
<ol>
<li>时间片 t 的计算依赖 t - 1 时刻的计算结果，限制了模型的并行能力，训练以及推理的过程慢；</li>
<li>尽管 LSTM 多个门和双状态输入的机制已经从一定程度上缓解了长依赖的问题，但缓解不意味着解决，对于特别长的依赖，LSTM 依旧无能为力，意味着顺序计算过程中信息还是会流失</li>
</ol>
<p>2017 年，重量级的 attention 论文发布，摒弃了传统的 NN 构型，只需要采用 attention 即可完成任务，同时 Encoder-decoder 被替换 —— 这就是著名的 Transformer 机制。</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>Transformer 沿用了 Encoder-decoder 的架构，但是将原来依赖于 RNN、LSTM 的 encoder 和 decoder 采用 attention 机制完全改写。</p>
<p>改写之后，一个完整的 transformer 由 encoder block 和 decoder block 两部分组成：</p>
<ul>
<li>每个 block 中的 encoder 由 self-attention 和 FNN(Feedforward Neural Network，前馈神经网络) 两部分组成；</li>
<li>每个 block 中的 decoder 同样包括 FNN，另外还有两个 self-attention，分别用于计算输入和输出的权值：<ul>
<li>Masked attention: 当前翻译和已经翻译的前文之间的关系，获取特征向量；</li>
<li>Encoder-decoder self-attention: 当前翻译和编码的特征向量之间的关系</li>
</ul>
</li>
</ul>
<p><img src="/2025/02/17/llm-intro/The-transformer-encoder-decoder-model.png" alt></p>
<p>encoder 和 decoder 都是以滑动窗口的方式分批次处理长文本的。</p>
<p>用户输入经过 transformer 得到了输出结果，是一个完整的端到端的流程。</p>
<h2 id="核心：self-attention"><a href="#核心：self-attention" class="headerlink" title="核心：self-attention"></a>核心：self-attention</h2><p>与传统 attention 的区别：</p>
<ol>
<li>传统 attention 基于 source 端和 target 端传递的大变量 hidden state 做计算，得到的结果代表了 source 端的每个词语 target 端每个词之间的依赖关系；</li>
<li>而 self-attention <strong>分别从 source 和 target 两端进行</strong>：<ul>
<li>先捕捉 source 端或 target 端自身的词与词之间的依赖关系；</li>
<li>再将 source 端得到的 self-attention 加入到 target 端得到的 attention，捕捉 source 端和 target 端词与词之间的依赖关系</li>
</ul>
</li>
</ol>
<p><img src="/2025/02/17/llm-intro/Attention_diagram_transformer.webp" alt></p>
<blockquote>
<p>self-attention 明确了三种不同的向量：查询向量 Q（query）、键向量 K（key）、值向量 V（value）。</p>
</blockquote>
<p>self-attention 比传统的 attention 机制效果要好，因为传统 attention 忽略了 source 或 target 端词与词之间的依赖关系；而 self-attention 不仅仅得到 source 和 target 端词与词之间的依赖关系，同时还可以有效获得 source 端或 target 端自身词与词之间的依赖关系。</p>
<p>在 self-attention 中：</p>
<ul>
<li>encoder 的输入为 inputs 结合 positional encoding，decoder 的输入为 outputs 结合其 positional encoding；</li>
<li>最简单的 self-attention 由一个 encoder block 和一个 decoder block 组成，每个 block 中的 attention 都是 <strong>Multi-head attention</strong>，分别由多个结构类似的 Scaled Dot-product attention 组成。</li>
</ul>
<p>Scaled Dot-product attention，缩放点乘注意力机制，跟上面讲的 attention 机制没啥区别，只是除去了维度的平方根，相对于 softmax 参数值较大时产生梯度消失的时候，可以使值的分布重新均匀，有效缓解梯度消失（关于梯度的计算，attention 机制的计算尤为复杂，本文先按下不表）。</p>
<p>回过来讲多头注意力机制：简单来说，就是多个 scaled dot-product attention 的结果进行连接后再次点乘权重；每个 scaled dot-product attention 称为一个“头”（head）。</p>
<h2 id="FNN"><a href="#FNN" class="headerlink" title="FNN"></a>FNN</h2><p>全连接层，包括第一层的非线性激活函数 relu 和第二层的线性激活函数：</p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="37.831ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 16721.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"/></g><g data-mml-node="mi" transform="translate(749,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="mi" transform="translate(1637,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="mo" transform="translate(2525,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(2914,0)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"/></g><g data-mml-node="mo" transform="translate(3637,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(4303.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(5359.6,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(6237.6,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mi" transform="translate(6766.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mo" transform="translate(7338.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mn" transform="translate(7727.6,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/></g><g data-mml-node="mo" transform="translate(8227.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(8672.2,0)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"/></g><g data-mml-node="msub" transform="translate(9395.2,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mo" transform="translate(10998,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="msub" transform="translate(11998.2,0)"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"/></g><g data-mml-node="mn" transform="translate(462,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mo" transform="translate(12863.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="msub" transform="translate(13252.8,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g><g data-mml-node="mo" transform="translate(14855.5,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="msub" transform="translate(15855.8,0)"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"/></g><g data-mml-node="mn" transform="translate(462,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g></g></g></svg></mjx-container></p>
<p>FNN 的存在变换了 attention output 的空间，增加了模型的表现能力；去掉 FNN 也可以，但是效果会变差。</p>
<h2 id="输入预处理"><a href="#输入预处理" class="headerlink" title="输入预处理"></a>输入预处理</h2><p>Transformer 的输入预处理包括两点：masking 机制、位置编码</p>
<p>Masking 机制有两类：用于处理非定长序列的 padding mask，和用于防止标签泄露的 sequence mask。</p>
<ul>
<li>padding mask 针对非定长序列切割成长短不一的文本 batch 后，针对长度不足的样本<strong>使用特殊字符（如 \<pad\> 或其他统一的特殊字符）</pad\></strong> 补全形成输入矩阵，再利用 <strong>Mask 矩阵（1 表示有效字，0 代表无效字）</strong> 生成输入；</li>
<li>sequence mask 用于在同时使用上下文信息的机制中遮盖要预测的标签信息，不让模型“提前看到”</li>
</ul>
<p><img src="/2025/02/17/llm-intro/padding%20mask.png" alt></p>
<p>padding mask 常常用在最终结果输出、损失函数计算等一切受样本实际长度影响的计算，或者不需要无用 padding 参与计算的时候。</p>
<p>再来说说位置编码（Position Encoding）。位置编码主要拿来确定句子中单词的语序信息：</p>
<ol>
<li>对于任何一门语言，单词在句子里面的位置是十分重要的。一个单词在句子里面的排列顺序不同，整句话的意思就可能发生偏差；</li>
<li>Transformer 抛弃了 RNN、CNN 这些顺序结构为基础的神经网络，以 attention 取而代之，词序就会丢失，模型就无法知道每个词在句子里面的相对和绝对位置</li>
</ol>
<h2 id="优势及后续发展"><a href="#优势及后续发展" class="headerlink" title="优势及后续发展"></a>优势及后续发展</h2><p>Transformer 在 NLP 的发展中优势巨大的原因是：</p>
<ol>
<li>模型并行度高，使训练时间大为降低；</li>
<li>可以直接捕获序列中的长距离依赖关系，不仅是输入与输出之间词与词的关系，也包括输入与输出本身词与词之间的依赖；</li>
<li>可以产生更具解释性的模型</li>
</ol>
<p>针对 transformer 吃内存等一系列局限性，业界有了以下优化：</p>
<ol>
<li>多查询注意力机制（Multi-Query Attention, MQA，用于 GPT-4）：只需要每个 head 有独立的 Q，K V 均共享，由此减少了需要加载的矩阵内存量；</li>
<li>ByteTransformer：因为可变长的序列非常普遍，切割成 batch 的时候会产生巨量没有意义的 padding，造成资源浪费；byteTransformer 解决了这个问题，不仅减少了内存应用，还维持了性能；</li>
<li>SparseTransformer：针对图像和视频等更高密度计算，运用稀疏矩阵进行的 transformer 优化</li>
<li>Decoder-only transformer：只保留了 decoder block 的 transformer，将原本需要翻译的文本（encoder 输入）和翻译完成的上文（decoder 输入）合并作为一个整体输入来处理（target-specific embedding 替代 encoder 的输入）。</li>
</ol>
<p><img src="/2025/02/17/llm-intro/decoder-only.png" alt></p>
<p>⚠️ 注意：<strong>现在的 LLM 都是 decoder-only 的架构</strong>。</p>
<p>encoder-decoder 需要在一定量的标注数据上做 multitask finetuning 才能激发最佳性能；而 decoder-only 在没有任何 tuning 数据的情况下，zero-shot（零样本学习）表现最好 —— 目前 LLM 训练还是基于大规模语言材料进行自监督学习，zero-shot 具有极大便利性。</p>
<h1 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h1><p>Pre-trained Models，预训练，即先在一个基础数据集上进行任务训练，生成一个<strong>基础网络</strong>，然后将学习到的特征进行微调，或者迁移到另一个目标网络上，用来训练新目标任务。</p>
<p>预训练是在大量常规数据集上学习数据里面的“<strong>共性</strong>”，然后在特定领域的少量标注数据学习“<strong>特性</strong>”。因此模型只需要从“共性”出发，学习特定任务“特性”即可。优势在于：</p>
<ul>
<li>预训练模型的参数从大量常规数据集中得来，比起单纯在自己的数据集上从头开始训练，进度会更快；</li>
<li>预训练模型更好地学到了数据中的普遍特征，比起在自己的数据集上从头开始训练会有更好的泛化效果</li>
</ul>
<p>怎么训练？这里就涉及到了一个重要的概念：</p>
<h2 id="有监督学习与无监督学习"><a href="#有监督学习与无监督学习" class="headerlink" title="有监督学习与无监督学习"></a>有监督学习与无监督学习</h2><p>有监督学习（Supervised Learning）有明确的目的，也明确知道期待的结果；因为训练过程中数据集有明确标签，所以中间计算错误了能直接判断出来，结果很容易被量化和衡量。</p>
<p>无监督学习（Unsupervised Learning）没有明确的目的，无法提前知道结果，因此也不完全明确训练数据的关系。其目标是<strong>找到数据集的底层结构，根据相似性对这个数据集进行内在关联和区分</strong>。</p>
<p><img src="/2025/02/17/llm-intro/Supervised_and_unsupervised_learning.png" alt></p>
<p>早期训练以有监督学习为主，但模型泛化性较差；而无监督学习有着更好的泛化效果，有助于从数据中找到“有用的见解”，思考方式也<strong>更接近于人类的思考方式</strong>，所以无监督学习的潜在能力更强。</p>
<p>基于训练集中数据的类型（和测试集类型是否相同），又可以分成以下的不同方式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th>中文名词</th>
<th>训练集中与测试集同类别的样本数量</th>
<th>例子</th>
</tr>
</thead>
<tbody>
<tr>
<td>Traditional learning</td>
<td>传统学习</td>
<td>&gt;&gt;1</td>
<td>测试集为一张斑马跑步的照片，训练集中有海量（比如百万的量级）有关斑马的标注的数据，有斑马休息的、几只斑马在一起的、斑马吃草的等等，也很可能有斑马各种跑步姿势的照片</td>
</tr>
<tr>
<td>Few-shot learning</td>
<td>少样本学习</td>
<td>&gt;1</td>
<td>测试集为一张斑马跑步的照片，训练集中有少量（比如数十的量级）有关斑马的标注的数据，有斑马休息的、几只斑马在一起的、斑马吃草的等等，极少和测试任务的照片相似</td>
</tr>
<tr>
<td>One-shot learning</td>
<td>单样本学习</td>
<td>=1</td>
<td>测试集为一张斑马跑步的照片，训练集中只有一张有关斑马的标注的数据，例如一张斑马休息的照片</td>
</tr>
<tr>
<td>Zero-shot learning</td>
<td>零样本学习</td>
<td>=0</td>
<td>测试集为一张斑马跑步的照片，训练集中没有有关斑马的标注的数据</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/2025/02/17/llm-intro/Zero-Shot-Learning-in-NLP-Modulai.webp" alt></p>
<p>很容易看出，zero-shot 泛化性更强，而传统方法已经基本不在用了。</p>
<h2 id="自监督学习"><a href="#自监督学习" class="headerlink" title="自监督学习"></a>自监督学习</h2><p>Self-supervides Learning，有监督和无监督两种学习方式的混用，即一种具有监督形式的无监督学习方法，模型在学习过程中进行自我监督，而不是通过前置知识来诱发。</p>
<p>同样，自监督学习没有预设的标签，而是使用数据集本身的信息构造标签，即通过构造辅助任务（pretask）完成学习。该方法衍生出来的 transformer 被命名为  Bidirectional Encoder Representations from Transformers（<strong>BERT</strong>），被广泛应用于现代大模型的预训练中。</p>
<h1 id="再谈谈-GPT"><a href="#再谈谈-GPT" class="headerlink" title="再谈谈 GPT"></a>再谈谈 GPT</h1><p>毕竟 DeepSeek 从各方面对标的就是目前大模型中的翘楚 —— GPT，我们聊完深度学习基础后，首先深入浅出地了解一下 GPT。</p>
<p>GPT 的全称是 Generative Pre-trained Transformer。</p>
<h2 id="GPT-1"><a href="#GPT-1" class="headerlink" title="GPT-1"></a>GPT-1</h2><p>12 层（个）decoder-only transformer，单头维度 768，参数量 0.117B。</p>
<ul>
<li>采用无监督预训练 + <strong>有监督微调</strong>的方式，因此不是完全的 zero-shot；</li>
<li>其无监督预训练基于语言模型进行，模型参数使用 SGD（Stochastic Gradient Descent, 随机坡度下降）进行优化。</li>
</ul>
<p>GPT 的参数中有一个叫 <strong>token</strong>，由一个特定维度大小的向量组成，类似于 Lucene 的分词器：一段话里面，如果能够分成 5 个单位（单词、标点符号、英文句子的字母等），意味着这句话消耗了 5 个 token；不同的模型支持的上下文长度不一样，有的 2048，有的 200k。</p>
<p>另外，GPT 采用多任务学习（Multi-task Learning）的方向，多个不同目标和不同结构的任务并行处理，有效提升了泛化性。</p>
<p><img src="/2025/02/17/llm-intro/multask.png" alt></p>
<h2 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a>GPT-2</h2><p>48 层 decoder-only transformer，单头维度 1600，参数量 1.5B。</p>
<p>GPT-2 其实在模型和架构上没有可圈可点之处，主要贡献在于：<strong>随着模型参数和训练数据的海量增长，只用 zero-shot 也可以达到相当高的模型准确性</strong>，这个特点后来被总结为 scaling law。</p>
<p><img src="/2025/02/17/llm-intro/scaling-law.png" alt></p>
<h2 id="GPT-3"><a href="#GPT-3" class="headerlink" title="GPT-3"></a>GPT-3</h2><p>96 层 decoder-only transformer，单头维度 12888，参数量 175B，真正火起来的一代 GPT，相比较于前两代更为强大。</p>
<p>重要的技术点在于 meta learning 和 in-context learning</p>
<blockquote>
<p>meta learning：元学习，针对传统机器学习中由人设计的学习算法，改成<strong>由机器设计学习算法</strong>，即根据学习资料确定学习函数。</p>
<p>meta learning 的一种实现叫 MAML（Model-Agnostic Meta-Learning），将输入 transformer 的 batch 再分为 support set 和 query set，类似于模型学习时的训练集和测试集。</p>
<p>meta learning 和预训练的差异在于：预训练更关注于当前任务的准确性，而元学习更加关注潜力，关注未来，二者梯度的更新方向不同。</p>
</blockquote>
<p><img src="/2025/02/17/llm-intro/meta-learning.jpg" alt></p>
<blockquote>
<p>in-context learning：上下文学习，指在不进行参数更新的情况下，只<strong>在输入中加入几个示例</strong>就能让模型进行学习。</p>
<p>in-context learning 属于 GPT-2 的扩展。</p>
</blockquote>
<p><img src="/2025/02/17/llm-intro/in-context.jpg" alt></p>
<p>GPT-3 中介绍的 in-context learning 是作为 meta learning 的内循环出现的，而基于 SGD 的预训练则是外循环。</p>
<h2 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h2><h3 id="提示学习与指示学习"><a href="#提示学习与指示学习" class="headerlink" title="提示学习与指示学习"></a>提示学习与指示学习</h3><p>两者的目的都是去挖掘语言模型本身具备的知识，不同的是：</p>
<ul>
<li>Prompt Learning，提示学习，激发语言模型的补全能力（完形填空、上半句生下半句等）；in-context learning 被认为是提示学习的一种；</li>
<li>Instruction Learning，指示学习，激发语言模型的理解能力，通过<strong>给出更明显的指令</strong>让模型做出正确的行动</li>
</ul>
<p><img src="/2025/02/17/llm-intro/prompt.png" alt></p>
<p>Instruction 比 prompt 更具泛化能力，在经过多任务微调后还能做 zero shot。</p>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>训练得到的模型可以看做是训练集的一个拟合。因为此时得到的模型并不非常可控，那么<strong>训练数据的分布</strong>就是影响生成内容质量的最重要因素。</p>
<p>有的时候我们希望模型不仅仅是受到训练数据的影响，而是人为可控的，从而保证生成数据的有用性、真实性、无公害性，因此我们需要需要人为的介入，目前 GPT 介入的方式就是<strong>奖励</strong>（<strong>Reward</strong>）机制，带奖励机制的学习方式就是<strong>强化学习</strong>（Reinforcement Learning, RL）。</p>
<p><img src="/2025/02/17/llm-intro/reinforcement-learning.png" alt></p>
<p>奖励机制可以被看作是传统模型训练机制的损失函数，其计算要比损失函数更灵活和多样；带来的代价是奖励计算不可导，因此不能拿来反向传播；同样人类反馈也是不可导的，故人工反馈也可作为强化学习的奖励。</p>
<p>强化学习通过对奖励的大量采样来拟合损失函数，以最大化长期累积奖励，从而实现模型的训练。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>综上，基于 GPT-3 的 ChatGPT 通过<strong>指示学习</strong>构建训练样本，以此来训练一个反映预测内容效果的<strong>奖励模型（Reward Model）</strong>，最后通过模型的打分来指导强化学习模型的训练。步骤如下：</p>
<ol>
<li>根据采集的数据集对 GPT-3 进行有监督微调（Supervised Fine-Tune, SFT）；</li>
<li>收集人工标注的对比数据，训练奖励模型（Reward Model, RM）；</li>
<li>使用 RM 作为强化学习的优化目标，利用 PPO（Proximal Policy Optimization，最近策略优化，可在多个训练步骤实现小批量的更新，解决了 Policy Gradient 算法中步长难以确定的问题）微调 SFT 模型</li>
</ol>
<h2 id="算力"><a href="#算力" class="headerlink" title="算力"></a>算力</h2><p>综上我们很容易得知，堆 transformer 的做法势必会带来算力要求的飞速增长。</p>
<h1 id="DeepSeek"><a href="#DeepSeek" class="headerlink" title="DeepSeek"></a>DeepSeek</h1><p>DeepSeek 泛指且不限于以下的模型：</p>
<ul>
<li>DeepSeek-V3：对话模型，最新的 DeepSeek 底座</li>
<li>DeepSeek-R1：推理模型，准确率相较于 DeepSeek V3更高，但思考过程过长</li>
<li>DeepSeek-R1-zero：推理模型，DeepSeek-R1 的先验版本，验证了 RL 本身对于激励 base 模型产生推理的能力</li>
<li>DeepSeek-R1-Distill-XXX-XXB：知识蒸馏版的推理模型。以 DeepSeek-R1-Distill-Qwen-7B 为例，使用 DeepSeek R1 中间阶段的训练数据，对 Qwen 2.5（参数量为 7B）进行 SFT 指令微调的模型。</li>
</ul>
<p>DeepSeek 总的特点在于，它完全抛开了预设的思维链模板（Chain of Thought）和监督式微调（SFT），仅依靠简单的奖惩信号（准确性奖励 <code>&lt;think&gt;</code> 和格式奖励 <code>&lt;answer&gt;</code>）来优化模型行为。</p>
<p>就本人肤浅的观察来看，DeepSeek 能够以 10% 不到的资源达到众多顶尖大模型的水平，采用的关键技术有两个：</p>
<h2 id="MLA"><a href="#MLA" class="headerlink" title="MLA"></a>MLA</h2><p>Multi-head Latent Attention，多头潜在注意力机制，通过动态合并（压缩 + 复用）相邻层的特征（K V）来减少计算量。</p>
<p><img src="/2025/02/17/llm-intro/mla.jpg" alt></p>
<p>同样的降本思路还在于 FP8，牺牲了一些精度来换取效率。</p>
<p>属实是：</p>
<blockquote>
<p>既然 GPU 少，那就卷工程，DeepSeek 这回确实是用东亚魔法打破西方垄断。</p>
</blockquote>
<h2 id="MoE"><a href="#MoE" class="headerlink" title="MoE"></a>MoE</h2><p>Mixture of Experts，混合专家模型，通过训练多个专家模型，每个专家针对特定的数据分布或任务进行优化。通过门控机制动态选择最合适的专家模块进行处理，从而提高模型的推理能力和效率。</p>
<p>与多任务学习（一个模型内多个学习任务）不同，MoE 是利用多个专家模型处理输入数据的不同方面或模式，通过门控网络决定如何将输入分配给各个专家，以及如何加权各位专家的输出。</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762">Attention Is All You Need</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/667508104">从 0 到 1 理解 GPT3</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg5ODAzMTkyMg==&amp;mid=2247485860&amp;idx=1&amp;sn=e926a739784090b3779711164217b968&amp;chksm=c06981f9f71e08efb5f57441444f71a09f1d27fc667af656a5ad1173e32ad394201d02195a3a&amp;mpshare=1&amp;scene=1&amp;srcid=0618HMAYi4gzzwWfedLoOuSD&amp;key=cb6098335ab487a8ec84c95399379f16f975d33ce91588d73ecf857c54b543666b5927e231ad3a9b17bff0c20fff20fc49c262912dca050dee9465801de8a4cdc79e3d8f4fbc058345331fb691bcbacb&amp;ascene=1&amp;uin=MTE3NTM4MTY0NA%3D%3D&amp;devicetype=Windows+10&amp;version=62060833&amp;lang=zh_CN&amp;pass_ticket=ikhBXxX7PL%2Fal9hbIGXbRFA96ei74EF%2BcP8KdbP6UcV6mIpOfPWzVuju%2Bqw86q5r">动画图解 Attention 机制，让你一看就明白</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2204.05832">What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.03400">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf">DeepSeek-V3 Technical Report</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Jcloud/p/18712775">🧐 DeepSeek-R1 原理解析及项目实践（含小白向概念解读）</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mjc1NjM3MjY2MA==&amp;mid=2691554386&amp;idx=1&amp;sn=41ffc5d3a2438583dc89f64b0c83d70f">省钱也是技术活：解密 DeepSeek 的极致压榨术</a></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"># 神经网络</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/03/09/transport-layer/" rel="prev" title="传输层">
      <i class="fa fa-chevron-left"></i> 传输层
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.</span> <span class="nav-text">深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%99%AE%E9%80%9A%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.1.</span> <span class="nav-text">普通的神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E9%A2%84%E5%A4%84%E7%90%86-pre-processing"><span class="nav-number">1.2.</span> <span class="nav-text">输入预处理 pre-processing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-activation-function"><span class="nav-number">1.3.</span> <span class="nav-text">激活函数 activation function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-loss-function"><span class="nav-number">1.4.</span> <span class="nav-text">损失函数 loss function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E8%BF%87%E5%BA%A6%E6%8B%9F%E5%90%88-overfitting"><span class="nav-number">1.5.</span> <span class="nav-text">解决过度拟合 overfitting</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NLP"><span class="nav-number">2.</span> <span class="nav-text">NLP</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN"><span class="nav-number">2.1.</span> <span class="nav-text">RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM"><span class="nav-number">2.2.</span> <span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Encoder-decoder"><span class="nav-number">2.3.</span> <span class="nav-text">Encoder-decoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention"><span class="nav-number">2.4.</span> <span class="nav-text">Attention</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer"><span class="nav-number">3.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9E%B6%E6%9E%84"><span class="nav-number">3.1.</span> <span class="nav-text">架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%EF%BC%9Aself-attention"><span class="nav-number">3.2.</span> <span class="nav-text">核心：self-attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FNN"><span class="nav-number">3.3.</span> <span class="nav-text">FNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">3.4.</span> <span class="nav-text">输入预处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8A%BF%E5%8F%8A%E5%90%8E%E7%BB%AD%E5%8F%91%E5%B1%95"><span class="nav-number">3.5.</span> <span class="nav-text">优势及后续发展</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="nav-number">4.</span> <span class="nav-text">预训练</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.1.</span> <span class="nav-text">有监督学习与无监督学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.2.</span> <span class="nav-text">自监督学习</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%86%8D%E8%B0%88%E8%B0%88-GPT"><span class="nav-number">5.</span> <span class="nav-text">再谈谈 GPT</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GPT-1"><span class="nav-number">5.1.</span> <span class="nav-text">GPT-1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPT-2"><span class="nav-number">5.2.</span> <span class="nav-text">GPT-2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPT-3"><span class="nav-number">5.3.</span> <span class="nav-text">GPT-3</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ChatGPT"><span class="nav-number">5.4.</span> <span class="nav-text">ChatGPT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%90%E7%A4%BA%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%8C%87%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="nav-number">5.4.1.</span> <span class="nav-text">提示学习与指示学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">5.4.2.</span> <span class="nav-text">强化学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-number">5.4.3.</span> <span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E5%8A%9B"><span class="nav-number">5.5.</span> <span class="nav-text">算力</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DeepSeek"><span class="nav-number">6.</span> <span class="nav-text">DeepSeek</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MLA"><span class="nav-number">6.1.</span> <span class="nav-text">MLA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MoE"><span class="nav-number">6.2.</span> <span class="nav-text">MoE</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">7.</span> <span class="nav-text">Reference</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">敖惠竣｜Ao Huijun, Raymond</p>
  <div class="site-description" itemprop="description">Storing blogs</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">82</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">35</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/daca-ao" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;daca-ao" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/realkaije" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;realkaije" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/daca.aohuijun" title="FB Page → https:&#x2F;&#x2F;www.facebook.com&#x2F;daca.aohuijun" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i>FB Page</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/kaijesugardaddy" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;kaijesugardaddy" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">敖惠竣｜Ao Huijun, Raymond</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
